import csv
import subprocess
import folium
import logging
from pathlib import Path
from turtle import pd


def normalize_mac(mac: str) -> str:
    """
    Normalize a MAC address by removing semicolons and colons,
    then converting it to lowercase.
    """
    return mac.replace(";", "").replace(":", "").strip().lower()


def results_to_csv(results_dir: Path, temp_output: str = "tmpresults.csv", master_output: str = "results.csv") -> None:
    """
    Runs hcxpcapngtool on all .pcapng files in the results_dir via a wildcard,
    generating a temporary CSV file. It then extracts specific columns
    (Date, Time, BSSID, SSID, Encryption, Latitude, Longitude) from the temporary CSV,
    and merges them into a master CSV file (appending without duplicates).

    Duplicate rows are skipped based on the normalized BSSID and SSID.

    :param results_dir: Path to the directory containing the .pcapng files.
    :param temp_output: Name for the temporary CSV file generated by hcxpcapngtool.
    :param master_output: Name for the master CSV file to create/update.
    """
    # shell expands wildcard & suppresses cli output
    cmd = f"hcxpcapngtool --csv={temp_output} *.pcapng"
    subprocess.run(cmd, shell=True, cwd=results_dir, check=True,
                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    master_csv = results_dir / master_output
    temp_csv_path = results_dir / temp_output
    seen = set()  # Set to track (BSSID, SSID) pairs already in master CSV.
    new_rows = []

    # if results.csv exists, load pairs
    if master_csv.exists():
        with open(master_csv, 'r', newline='') as f:
            reader = csv.reader(f)
            header = next(reader, None)
            for row in reader:
                if len(row) >= 4:
                    bssid_norm = normalize_mac(row[2])
                    ssid_norm = row[3].strip().lower()
                    seen.add((bssid_norm, ssid_norm))

    # process tmp results
    with open(temp_csv_path, 'r', newline='') as f:
        reader = csv.reader(f, delimiter='\t')
        for row in reader:
            # Ensure row has at least 16 columns.
            if len(row) < 16:
                continue
            date = row[0]
            time_val = row[1]
            bssid = row[2]
            ssid = row[3]
            encryption = " ".join([row[4], row[5], row[6]]).strip()
            latitude = row[14]
            longitude = row[15]

            # Normalize bssid and ssid before comparing
            key = (normalize_mac(bssid), ssid.strip().lower())
            if key in seen:
                continue
            seen.add(key)
            new_rows.append([date, time_val, bssid, ssid, encryption, latitude, longitude])

    # Remove tmp file
    temp_csv_path.unlink()

    # write/append to results.csv
    write_header = not master_csv.exists()
    with open(master_csv, 'a', newline='') as f:
        writer = csv.writer(f)
        if write_header:
            writer.writerow(['Date', 'Time', 'BSSID', 'SSID', 'Encryption', 'Latitude', 'Longitude'])
        for row in new_rows:
            writer.writerow(row)


def append_keys_to_results(results_csv: Path, founds_txt: Path) -> None:
    """
    Compares bssid:ssid pairs in founds.txt (lines formatted as
    "bssid:randomvalue:ssid:key") with the corresponding values in results.csv.
    If a match is found, the key from founds.txt is appended (or updated) in results.csv.

    The MAC addresses are normalized (lowercase and semicolons removed) before comparison.

    :param results_csv: Path to the results CSV file.
    :param founds_txt: Path to the founds.txt file from WPA-sec.
    """
    # Build mapping from founds.txt: (bssid, ssid) -> key
    founds_map = {}
    try:
        with open(founds_txt, 'r') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                parts = line.split(':')
                if len(parts) != 4:
                    continue
                raw_bssid, random_val, raw_ssid, key_val = parts
                bssid = normalize_mac(raw_bssid)
                ssid = raw_ssid.strip().lower()  # Normalizing the ssid too
                founds_map[(bssid, ssid)] = key_val
    except Exception as e:
        logging.error(f"Error reading {founds_txt}: {e}")
        return

    # Read the current results.csv into memory.
    rows = []
    header = []
    try:
        with open(results_csv, 'r', newline='') as csvfile:
            reader = csv.reader(csvfile)
            header = next(reader)
            rows = list(reader)
    except Exception as e:
        logging.error(f"Error reading {results_csv}: {e}")
        return

    # Check if "Key" column exists; if not, add it.
    if "Key" not in header:
        header.append("Key")
        key_index = len(header) - 1
        # Extend every row to include an empty key value.
        for row in rows:
            row.append("")
    else:
        key_index = header.index("Key")

    updated_count = 0
    # Assuming results.csv has BSSID at index 2 and SSID at index 3.
    for row in rows:
        if len(row) < 4:
            continue
        csv_bssid = normalize_mac(row[2])
        csv_ssid = row[3].strip().lower()
        match_key = founds_map.get((csv_bssid, csv_ssid))
        if match_key:
            if len(row) <= key_index or row[key_index] != match_key:
                if len(row) <= key_index:
                    row.extend([""] * (key_index - len(row) + 1))
                row[key_index] = match_key
                updated_count += 1

    # Write the updated CSV back.
    try:
        with open(results_csv, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(header)
            writer.writerows(rows)
        logging.error(f"Updated {updated_count} rows in {results_csv}.")
    except Exception as e:
        logging.error(f"Error writing {results_csv}: {e}")


def create_html_map(results_csv: Path, output_html: str = "map.html") -> None:
    """
    Reads the results CSV and creates an interactive map (using Folium)
    with a marker for each row. The marker popup displays row data.

    :param results_csv: Path to the results CSV file.
    :param output_html: The name of the HTML file to output.
    """
    # Read the CSV file using pandas.
    try:
        df = pd.read_csv(results_csv)
    except Exception as e:
        logging.error(f"Error reading {results_csv}: {e}")
        return

    # Ensure that Latitude and Longitude are floats.
    try:
        df["Latitude"] = df["Latitude"].astype(float)
        df["Longitude"] = df["Longitude"].astype(float)
    except Exception as e:
        logging.error(f"Error converting coordinates: {e}")
        return

    # Compute a central point for the map.
    if not df.empty:
        avg_lat = df["Latitude"].mean()
        avg_lon = df["Longitude"].mean()
    else:
        avg_lat, avg_lon = 0, 0

    # Create the map.
    m = folium.Map(location=[avg_lat, avg_lon], zoom_start=10)

    # Add markers.
    for _, row in df.iterrows():
        popup_content = (
            f"<strong>Date:</strong> {row['Date']}<br>"
            f"<strong>Time:</strong> {row['Time']}<br>"
            f"<strong>BSSID:</strong> {row['BSSID']}<br>"
            f"<strong>SSID:</strong> {row['SSID']}<br>"
            f"<strong>Encryption:</strong> {row['Encryption']}<br>"
            f"<strong>Key:</strong> {row.get('Key', '')}"
        )
        folium.Marker(
            location=[row["Latitude"], row["Longitude"]],
            popup=popup_content,
        ).add_to(m)
    # Save the map to an HTML file.
    m.save(results_csv.parent / output_html)
    logging.info(f"Map saved to {results_csv.parent / output_html}")
