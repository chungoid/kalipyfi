import csv
import sqlite3

import folium
import pandas
import logging
import subprocess
from pathlib import Path

# locals
from config.constants import BASE_DIR
from database.db_manager import get_db_connection
from tools.helpers.sql_utils import query_valid_hcxtool_entries
from tools.helpers.tool_utils import normalize_mac


def nmea_to_decimal(coord_str: str, direction: str) -> float:
    """
    Converts an NMEA coordinate string to a decimal degree value.

    For latitude (N/S), the coordinate is expected in ddmm.mmmmm format.
    For longitude (E/W), if the coordinate string is 11 characters long,
    it's assumed to be in ddmm.mmmmm format (i.e. missing a leading 0), otherwise
    it uses dddmm.mmmmm.

    :param coord_str: The raw coordinate string.
    :param direction: The direction ('N', 'S', 'E', or 'W').
    :return: The coordinate in decimal degrees.
    """
    try:
        if not coord_str or not direction:
            return 0.0

        if direction.upper() in ['N', 'S']:
            deg_digits = 2
        else:  # for 'E' or 'W'
            # If the string is 11 characters, assume it is ddmm.mmmmm
            if len(coord_str) == 11:
                deg_digits = 2
            else:
                deg_digits = 3

        degrees = float(coord_str[:deg_digits])
        minutes = float(coord_str[deg_digits:])
        decimal = degrees + minutes / 60.0
        if direction.upper() in ['S', 'W']:
            decimal = -decimal
        return decimal
    except Exception as e:
        logging.error(f"Error converting NMEA coordinate '{coord_str}' with direction '{direction}': {e}")
        return 0.0


def run_hcxpcapngtool(results_dir: Path, temp_output: str = "tmpresults.csv") -> Path:
    cmd = f"hcxpcapngtool --csv={temp_output} *.pcapng"
    logging.debug(f"Running command: {cmd} in {results_dir}")
    subprocess.run(cmd, shell=True, cwd=results_dir, check=True,
                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    return results_dir / temp_output

def parse_temp_csv(temp_csv_path: Path, master_output: str = "results.csv") -> Path:
    """
    Parses a temporary CSV file generated by hcxtool (with tab-delimited fields),
    converts the GPS coordinates from NMEA to decimal, and writes a master CSV.
    Also, inserts each valid row into the hcxtool_results database table.

    If the converted latitude or longitude equals 0.0, they are stored as NULL in the DB.

    Parameters:
        temp_csv_path (Path): The path to the temporary CSV file.
        master_output (str): The filename for the master CSV file.

    Returns:
        Path: The path to the master CSV file.
    """
    results_dir = temp_csv_path.parent
    master_csv = results_dir / master_output
    new_rows = []

    conn = get_db_connection(BASE_DIR)

    with open(temp_csv_path, 'r', newline='') as f:
        reader = csv.reader(f, delimiter='\t')
        for row in reader:
            if len(row) < 14:
                logging.debug(f"Skipping row (insufficient columns): {row}")
                continue

            date = row[0]
            time_val = row[1]
            bssid = normalize_mac(row[2])
            ssid = row[3]
            encryption = row[4]
            raw_lat = row[10]
            lat_dir = row[11]
            raw_lon = row[12]
            lon_dir = row[13]

            logging.debug(f"Raw GPS for {bssid}/{ssid}: lat='{raw_lat}' {lat_dir}, lon='{raw_lon}' {lon_dir}")
            latitude = nmea_to_decimal(raw_lat, lat_dir)
            longitude = nmea_to_decimal(raw_lon, lon_dir)
            logging.debug(f"Converted GPS for {bssid}/{ssid}: latitude={latitude}, longitude={longitude}")

            # If coordinates are 0.0, set NULL
            if latitude == 0.0:
                latitude = None
            if longitude == 0.0:
                longitude = None

            new_rows.append([date, time_val, bssid, ssid, encryption, latitude, longitude])
            # pass an empty string for 'key' ..add it later if user gets wpasec dl
            from tools.hcxtool.db import insert_hcxtool_results
            insert_hcxtool_results(conn, date, time_val, bssid, ssid, encryption, latitude, longitude, "")

    conn.close()
    # delete tmp
    try:
        temp_csv_path.unlink()
        logging.debug(f"Deleted temporary CSV: {temp_csv_path}")
    except Exception as e:
        logging.error(f"Could not delete temporary CSV {temp_csv_path}: {e}")

    # Write the master CSV file.
    with open(master_csv, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Date', 'Time', 'BSSID', 'SSID', 'Encryption', 'Latitude', 'Longitude'])
        for row in new_rows:
            writer.writerow(row)
    logging.info(f"Wrote {len(new_rows)} rows to master CSV {master_csv}")
    return master_csv

def read_founds(founds_txt: Path) -> dict:
    """Reads founds.txt and returns a dict keyed by (bssid, ssid) with key values."""
    founds_map = {}
    try:
        with open(founds_txt, 'r') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                parts = line.split(':')
                if len(parts) != 4:
                    continue
                raw_bssid = parts[0]
                raw_ssid = parts[2]
                key_val = parts[3]
                bssid = normalize_mac(raw_bssid)
                ssid = raw_ssid.strip().lower()
                founds_map[(bssid, ssid)] = key_val
        logging.debug(f"Constructed founds_map with {len(founds_map)} entries.")
    except Exception as e:
        logging.error(f"Error reading founds.txt: {e}")
    return founds_map

def read_master_csv(master_csv: Path) -> (dict, list):
    """Reads master CSV and returns a tuple: (data dict, header)."""
    data = {}
    header = ['Date', 'Time', 'BSSID', 'SSID', 'Encryption', 'Latitude', 'Longitude', 'Key']
    try:
        with open(master_csv, 'r', newline='') as f:
            reader = csv.reader(f)
            header = next(reader)
            if "Key" not in header:
                header.append("Key")
            key_index = header.index("Key")
            for row in reader:
                if len(row) < 4:
                    continue
                if len(row) < len(header):
                    row += [""] * (len(header) - len(row))
                csv_bssid = normalize_mac(row[2])
                csv_ssid = row[3].strip().lower()
                data[(csv_bssid, csv_ssid)] = row
    except Exception as e:
        logging.warning(f"Master CSV not found or could not be read ({e}). Starting with empty CSV.")
    return data, header

def merge_data(csv_data: dict, header: list, founds_map: dict) -> dict:
    """Merges CSV data with founds_map. Updates keys and adds missing entries."""
    key_index = header.index("Key")
    # update existing CSV data with founds keys
    for key_tuple, found_key in founds_map.items():
        if key_tuple in csv_data:
            csv_data[key_tuple][key_index] = found_key
        else:
            # create a new row with defaults
            new_row = ["", "", key_tuple[0], key_tuple[1], "", "", "", found_key]
            csv_data[key_tuple] = new_row
    logging.info(f"Merged total of {len(csv_data)} entries after combining CSV and founds.txt.")
    return csv_data

def write_master_csv(master_csv: Path, header: list, csv_data: dict) -> None:
    """Writes the merged data back to the master CSV."""
    try:
        with open(master_csv, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(header)
            for row in csv_data.values():
                writer.writerow(row)
        logging.info(f"Master CSV written with {len(csv_data)} entries.")
    except Exception as e:
        logging.error(f"Error writing master CSV: {e}")

def update_database(csv_data: dict, header: list) -> None:
    """Updates the hcxtool database with the merged data.

    For each row, if an entry with the same BSSID already exists (regardless of SSID),
    its values are updated. Otherwise, a new entry is inserted.
    Afterwards, cleanup_db is run using the same connection.
    """
    try:
        conn = get_db_connection(BASE_DIR)
        cursor = conn.cursor()

        # Define update and insert queries.
        update_query = """
            UPDATE hcxtool
            SET date = ?,
                time = ?,
                ssid = ?,
                encryption = ?,
                latitude = ?,
                longitude = ?,
                key = ?
            WHERE bssid = ?
        """
        insert_query = """
            INSERT INTO hcxtool (date, time, bssid, ssid, encryption, latitude, longitude, key)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """

        for row in csv_data.values():
            # row structure: [Date, Time, BSSID, SSID, Encryption, Latitude, Longitude, Key]
            try:
                lat = float(row[5]) if row[5] and row[5] != "0" else None
            except Exception:
                lat = None
            try:
                lon = float(row[6]) if row[6] and row[6] != "0" else None
            except Exception:
                lon = None

            # Try to update an existing entry based on bssid.
            cursor.execute(update_query, (row[0], row[1], row[3], row[4], lat, lon, row[7], row[2]))
            # If no rows were updated, then insert a new entry.
            if cursor.rowcount == 0:
                cursor.execute(insert_query, (row[0], row[1], row[2], row[3], row[4], lat, lon, row[7]))

        conn.commit()
        logging.info("Database updated with merged data from CSV and founds.txt.")

        # Run cleanup_db using the existing connection.
        #cleanup_db(conn)
        conn.commit()
        conn.close()
    except Exception as e:
        logging.error(f"Error updating the database with merged data: {e}")

def cleanup_db(conn: sqlite3.Connection) -> None:
    """
    Consolidates entries in the hcxtool database based solely on the bssid.
    For each bssid that appears in multiple rows:
      - If any record has a non-empty key, the one with the latest created_at is chosen.
      - Otherwise, the record with the latest created_at is chosen.
    All other records for that bssid are deleted and only the consolidated record is kept.
    This function uses the provided connection.
    """
    logger = logging.getLogger("cleanup_db")
    # Ensure we get rows as dictionaries.
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()

    # Retrieve all rows from the table.
    cursor.execute("SELECT * FROM hcxtool")
    rows = cursor.fetchall()

    if not rows:
        logger.info("No entries found in the database.")
        return

    # Group records by bssid.
    grouped = {}
    for row in rows:
        bssid = row["bssid"]
        grouped.setdefault(bssid, []).append(dict(row))

    # For each group (each unique bssid), consolidate if there are duplicates.
    for bssid, records in grouped.items():
        if len(records) <= 1:
            continue  # Only one record exists, no consolidation needed.

        logger.info(f"Consolidating {len(records)} records for BSSID {bssid}:")
        for rec in records:
            logger.debug(
                f"Record: date={rec['date']}, time={rec['time']}, ssid={rec['ssid']}, "
                f"encryption={rec['encryption']}, latitude={rec['latitude']}, "
                f"longitude={rec['longitude']}, key='{rec['key']}', created_at={rec.get('created_at')}"
            )

        # Prefer records with a non-empty key.
        records_with_key = [r for r in records if r.get("key", "").strip() != ""]
        if records_with_key:
            chosen = max(records_with_key, key=lambda r: r["created_at"])
            logger.info(
                f"Chosen record for BSSID {bssid} (with key, latest created_at): "
                f"date={chosen['date']}, time={chosen['time']}, ssid={chosen['ssid']}, "
                f"encryption={chosen['encryption']}, latitude={chosen['latitude']}, "
                f"longitude={chosen['longitude']}, key='{chosen['key']}', created_at={chosen.get('created_at')}"
            )
        else:
            chosen = max(records, key=lambda r: r["created_at"])
            logger.info(
                f"Chosen record for BSSID {bssid} (latest created_at): "
                f"date={chosen['date']}, time={chosen['time']}, ssid={chosen['ssid']}, "
                f"encryption={chosen['encryption']}, latitude={chosen['latitude']}, "
                f"longitude={chosen['longitude']}, key='{chosen['key']}', created_at={chosen.get('created_at')}"
            )

        # Delete all rows for this bssid.
        cursor.execute("DELETE FROM hcxtool WHERE bssid = ?", (bssid,))
        logger.info(f"Deleted all records for BSSID {bssid}.")

        # Insert the consolidated record back into the table.
        insert_query = """
            INSERT INTO hcxtool (bssid, date, time, ssid, encryption, latitude, longitude, key)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """
        cursor.execute(insert_query, (
            chosen["bssid"],
            chosen["date"],
            chosen["time"],
            chosen["ssid"],
            chosen["encryption"],
            chosen["latitude"],
            chosen["longitude"],
            chosen["key"]
        ))
        logger.info(f"Reinserted consolidated record for BSSID {bssid}.")

    logger.info("Database consolidation complete.")


def append_keys_to_master(master_csv: Path, founds_txt: Path) -> None:
    founds_map = read_founds(founds_txt)
    csv_data, header = read_master_csv(master_csv)
    merged_data = merge_data(csv_data, header, founds_map)
    write_master_csv(master_csv, header, merged_data)
    update_database(merged_data, header)

def db_to_html_map(results_dir: Path, output_html: str = "map.html") -> None:
    """
    Creates an HTML map with two layers from the hcxtool database entries:
      - "All Scans": shows every entry with valid latitude and longitude.
      - "Scans with Keys": shows only entries where the key field is non-empty.
    The map is saved as output_html (default "map.html") in the provided results_dir.
    """
    logger = logging.getLogger("db_to_html_map")

    # Get valid database entries
    results = query_valid_hcxtool_entries()
    if not results:
        logger.error("No valid entries found in the database.")
        return

    # Build a DataFrame from the query results
    df = pandas.DataFrame(results)
    logger.info(f"Retrieved {len(df)} entries from the database.")

    # Compute map center based on valid entries
    avg_lat = df["latitude"].mean()
    avg_lon = df["longitude"].mean()
    logger.info(f"Computed map center: ({avg_lat}, {avg_lon}).")
    m = folium.Map(location=[avg_lat, avg_lon], zoom_start=10)

    # Create feature groups for two layers
    fg_all = folium.FeatureGroup(name="All Scans", show=True)
    fg_keys = folium.FeatureGroup(name="Scans with Keys", show=False)

    # Iterate through DataFrame rows and add markers
    for index, row in df.iterrows():
        logger.info(
            f"Processing entry {index+1}: BSSID={row['bssid']}, SSID={row['ssid']}, "
            f"Coordinates=({row['latitude']}, {row['longitude']}), Key='{row['key']}'"
        )
        popup_content = (
            f"<strong>Date:</strong> {row['date']}<br>"
            f"<strong>Time:</strong> {row['time']}<br>"
            f"<strong>BSSID:</strong> {row['bssid']}<br>"
            f"<strong>SSID:</strong> {row['ssid']}<br>"
            f"<strong>Encryption:</strong> {row['encryption']}<br>"
            f"<strong>Key:</strong> {row['key']}"
        )
        marker_location = [row["latitude"], row["longitude"]]
        marker_all = folium.Marker(location=marker_location, popup=popup_content)
        fg_all.add_child(marker_all)

        # Add marker to "Scans with Keys" only if the key is non-empty
        if row["key"] and str(row["key"]).strip() != "":
            marker_key = folium.Marker(location=marker_location, popup=popup_content)
            fg_keys.add_child(marker_key)
            logger.debug(f"Added marker with key for {row['bssid']} at {marker_location}")

    # Add both layers and layer control to the map
    m.add_child(fg_all)
    m.add_child(fg_keys)
    m.add_child(folium.LayerControl())

    # Build full output path using the results directory
    output_path = Path(results_dir) / output_html
    try:
        m.save(output_path)
        logger.info(f"DB HTML map saved to {output_path}")
    except Exception as e:
        logger.error(f"Error saving DB HTML map: {e}")



